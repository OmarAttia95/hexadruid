{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2960674353e56349",
   "metadata": {},
   "source": [
    "# HexaDruid: Auto Fix Spark Skew  \n",
    "Welcome! This notebook will walk you through **HexaDruid**, our PySpark library that:\n",
    "\n",
    "-  **Automatically detects & corrects** data skew  \n",
    "-  **Infers schemas** and builds a **DRTree** for logical sharding  \n",
    "-  **Tunes parameters** (salt count & shuffle partitions)  \n",
    "-  Is designed to be **noob-friendly**, with plenty of tips  \n",
    "\n",
    "Let’s get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0b937a43f33a9",
   "metadata": {},
   "source": [
    "# Step 1: Spark Session Configuration\n",
    "\n",
    "Basic settings and performance tuners for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab18c319eaac5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:35:06.111658Z",
     "start_time": "2025-07-07T08:34:54.254680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required Python packages (only if not already installed)\n",
    "!pip install pyspark>=3.5.1 pandas matplotlib seaborn --quiet\n",
    "!pip install --upgrade hexadruid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6232c4dc0438c40",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries\n",
    "\n",
    "Now we bring in:\n",
    "\n",
    "- 🔧 **HexaDruid** core classes & functions  \n",
    "- 📊 **pandas** for local DataFrame previews  \n",
    "\n",
    "> 💡 *Tip:* Make sure you’ve installed your package in editable mode:  \n",
    "> ```bash\n",
    "> pip install -e .\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:36:12.288175Z",
     "start_time": "2025-07-07T08:36:12.279516Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Import Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from hexadruid import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbebd2a",
   "metadata": {},
   "source": [
    "> 💡 *Tip:* Make sure you pick the correct API methods to import from the package:\n",
    "- Kindly refer to the API references for clarity\n",
    "- Use `print(dir(hexadruid))` to call all the APIs, and select what to import!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hexadruid\n",
    "print(dir(hexadruid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f6fae159bdb54",
   "metadata": {},
   "source": [
    "# Step 3: Spark Session Configuration\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. **Create** a SparkSession with sensible local defaults  \n",
    "2. **Apply** performance-tuning configs for development  \n",
    "3. **Set** log level to WARN to avoid noise  \n",
    "\n",
    "> 💡 *Tip:* You can adjust `spark_driver_memory` and `spark_executor_memory` based on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf0222f6ad952c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:35:06.377507Z",
     "start_time": "2025-07-07T08:35:06.230313Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Spark Session Configuration\n",
    "# ----------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Basic Settings\n",
    "spark_app_name = \"HexaDruid_Test\"\n",
    "spark_master = \"local[*]\"\n",
    "spark_driver_memory = \"2g\"\n",
    "spark_executor_memory = \"2g\"\n",
    "spark_log_level = \"WARN\"\n",
    "\n",
    "# Performance Tuners\n",
    "spark_configs = {\n",
    "    \"spark.sql.shuffle.partitions\": \"4\",\n",
    "    \"spark.default.parallelism\": \"4\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "    \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
    "    \"spark.sql.broadcastTimeout\": \"60\",\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.codegen.wholeStage\": \"true\",\n",
    "    \"spark.ui.showConsoleProgress\": \"false\",\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Build Spark Session\n",
    "# ----------------------------\n",
    "\n",
    "builder = SparkSession.builder.appName(spark_app_name).master(spark_master)\n",
    "\n",
    "# Apply memory configs\n",
    "\n",
    "builder = builder.config(\"spark.driver.memory\", spark_driver_memory)\n",
    "builder = builder.config(\"spark.executor.memory\", spark_executor_memory)\n",
    "\n",
    "# Apply performance configs\n",
    "for k, v in spark_configs.items():\n",
    "    builder = builder.config(k, v)\n",
    "\n",
    "# Create Spark session\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "# Set log level\n",
    "spark.sparkContext.setLogLevel(spark_log_level)\n",
    "\n",
    "# Confirm Spark details\n",
    "print(f\"Spark v{spark.version} started | Driver: {spark_driver_memory} | Executor: {spark_executor_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888bcc7adf01fc8",
   "metadata": {},
   "source": [
    "# Step 3: Define Input Path\n",
    "\n",
    "**Action:** Update these two variables:\n",
    "\n",
    "- `input_path`: full path to your data file  \n",
    "- `file_type`: one of `\"csv\"`, `\"json\"`, `\"parquet\"`, `\"delta\"`  \n",
    "\n",
    "This controls how we load the data in Step 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d8481227b9358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:35:06.662491Z",
     "start_time": "2025-07-07T08:35:06.427811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace this path with your own CSV\n",
    "file_type = \"csv\"                 # Options: \"csv\", \"json\", \"parquet\", \"delta\"\n",
    "input_file = fr\"YOUR FILE HERE.{file_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f7590",
   "metadata": {},
   "source": [
    "# Step 4: Load Your Data (Any File Type)\n",
    "\n",
    "We support:\n",
    "\n",
    "- **`.csv`**: comma-separated values (all columns read as string)  \n",
    "- **`.json`**: JSON records (multiline or line-delimited)  \n",
    "- **`.parquet`**: Spark’s columnar format  \n",
    "- **`.delta`**: Delta Lake tables (optional)  \n",
    "\n",
    "We’ll **preview** the first 5 rows and print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_type == \"csv\":\n",
    "    df_raw = (\n",
    "        spark.read\n",
    "             .option(\"header\", \"true\")\n",
    "             .option(\"inferSchema\", \"false\")  # load all columns as strings\n",
    "             .csv(input_file)\n",
    "    )\n",
    "elif file_type == \"json\":\n",
    "    df_raw = spark.read.option(\"multiLine\", \"true\").json(input_file)\n",
    "elif file_type == \"parquet\":\n",
    "    df_raw = spark.read.parquet(input_file)\n",
    "elif file_type == \"delta\":\n",
    "    df_raw = spark.read.format(\"delta\").load(input_file)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file type: {file_type!r}\")\n",
    "\n",
    "print(f\"Loaded {file_type.upper()} data:\")\n",
    "df_raw.show(5, truncate=False)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b83218",
   "metadata": {},
   "source": [
    "# Step 5 (Optional): Infer Schema & Build DRTree\n",
    "\n",
    "*You can skip this if you already know your schema and don’t need sharding metadata.*\n",
    "\n",
    "This step:\n",
    "\n",
    "1. **Samples** a fraction of rows  \n",
    "   - Controlled by `sample_fraction` (default **20%**).  \n",
    "   - We “peek” at only part of the data to infer types and build the DRTree quickly, without scanning your entire dataset.\n",
    "\n",
    "2. **Infers** each column’s type (int, double, string)  \n",
    "   - Looks at the sampled values to decide if a column should be Integer, Double, or remain String.\n",
    "\n",
    "3. **Builds** a lightweight **DRTree** for logical data shards  \n",
    "   - Splits the data into branches based on skewed columns (see below).  \n",
    "   - The shape of the tree is governed by `max_depth` and `min_samples`.\n",
    "\n",
    "4. **Casts** the full DataFrame to the inferred schema  \n",
    "   - Applies the new schema to every row in the original DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter Details\n",
    "\n",
    "- `sample_fraction=0.2`  \n",
    "  Peek at **20%** of rows for inference (faster + lower memory).\n",
    "\n",
    "- `max_depth=3`  \n",
    "  Limit tree depth to **3 splits** (up to 2³ = 8 shards).\n",
    "\n",
    "- `min_samples=500`  \n",
    "  Stop splitting any shard with fewer than **500 rows** (avoid tiny shards).\n",
    "\n",
    "- `skew_thresh=0.1`  \n",
    "  Only numeric columns with skew score **> 0.10** are used to split.\n",
    "\n",
    "- `skew_top_n=3`  \n",
    "  Consider the **top 3** most-skewed numeric columns when building the tree.\n",
    "\n",
    "---\n",
    "\n",
    "**Outputs**:\n",
    "\n",
    "- `df_typed` — your original DataFrame cast to the inferred schema  \n",
    "- `schema`   — the `StructType` object describing each column  \n",
    "- `dr_tree`  — metadata (Roots & Branches) defining your logical shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ca237",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_schema = True  # ← set to False, if you need to skip the Smart Schema Detection!\n",
    "if do_schema:\n",
    "    df_typed, schema, dr_tree = HexaDruid.schemaVisor(\n",
    "        df_raw,\n",
    "        sample_fraction=0.2,\n",
    "        max_depth=3,\n",
    "        min_samples=500,\n",
    "        skew_thresh=0.1,\n",
    "        skew_top_n=3\n",
    "    )\n",
    "    print(\"Inferred StructType:\\n\", schema.json())\n",
    "    print(\"DRTree metadata:\\n\", dr_tree.to_dict())\n",
    "else:\n",
    "    df_typed = df_raw\n",
    "    print(\"Skipping schemaVisor; proceeding with raw DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1832f22952071a",
   "metadata": {},
   "source": [
    "# Step 6: Inspect Numeric Columns & Skew Detection\n",
    "\n",
    "- **Numeric columns**: which fields can be used for skew analysis  \n",
    "- **Skew detection**: top _N_ columns exceeding the skew threshold  \n",
    "\n",
    "---\n",
    "\n",
    "### Skew parameters explained\n",
    "\n",
    "- **`threshold=0.1`**  \n",
    "  Only columns whose **skew score** exceeds **0.10 (10%)** will be flagged as skewed.  \n",
    "  A skew score (based on quartiles) measures how unbalanced the distribution is—higher means more skew.\n",
    "\n",
    "- **`top_n=3`**  \n",
    "  Of all the columns that pass the threshold, pick the **top 3** worst-skewed columns for salting.  \n",
    "  This focuses your effort on the biggest offenders.\n",
    "\n",
    "> 💡 *Tip:*  \n",
    "> - If you detect **no** skewed columns, try **lowering** `threshold` (e.g. to `0.05`).  \n",
    "> - If you want to salt more columns, **increase** `top_n` (e.g. to `5`).  \n",
    "> - ALWAYS choose the number of columns wisely! **Proceed with extreme caution** (`2` as a default)\n",
    "> - Make sure to use the output of `Detected skewed columns` in step 8.\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff381eecfddc046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:35:06.837309Z",
     "start_time": "2025-07-07T08:35:06.716667Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) List numeric columns\n",
    "numeric_cols = HexaDruid.infer_numeric_columns(df_typed)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# 2) Detect skew among them\n",
    "skew_detector = SkewFeatureDetector(threshold = 0.1, top_n = 2)\n",
    "skewed = skew_detector.detect(df_typed)\n",
    "\n",
    "# Make sure you use this in your step (8)\n",
    "print(\"Detected skewed columns with indexes:\")\n",
    "for idx, col_name in enumerate(skewed):\n",
    "    if col_name in skewed:\n",
    "        print(f\"- Index: {idx}, Column: {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6205020fa4e682",
   "metadata": {},
   "source": [
    "# Step 7: Detect Low-Cardinality Categorical Column\n",
    "\n",
    "We need a **groupBy** column with few distinct values (≤20), ideal for counting.\n",
    "\n",
    "- The method picks the **first** such string column.  \n",
    "- Raises an error if none qualifies.\n",
    "- The variable `grp_col` is a pointer (a Python string) holding the name of the chosen column. \n",
    "\n",
    "> 💡 **Tips for Noobs:**  \n",
    "> - Verify that `grp_col` makes sense for your analysis—avoid grouping by high-cardinality fields (like user IDs or timestamps) which can reintroduce skew or create too many groups.  \n",
    ">\n",
    "> - Override the default anytime by specifying your own column in your code, for example:  \n",
    ">   ```python\n",
    ">   df.groupBy(\"your_column\").count()\n",
    ">   ```  \n",
    "> - If you see an error, inspect your DataFrame’s string columns to ensure at least one has **20 or fewer** distinct values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fd0b988ed1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_col = HexaDruid.detect_low_cardinality_categorical(df_typed)\n",
    "\n",
    "print(\"Low-cardinality categorical column:\", grp_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708d0b9ce53f662",
   "metadata": {},
   "source": [
    "# Step 8: Apply Smart Salting\n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Salt** (bucket) the top-skew column into `salt_count` buckets  \n",
    "2. **Visualize**: z-score, boxplot, histogram (saved to `output_dir`)  \n",
    "3. **Fine-tune** salt count on 10% sample  \n",
    "4. **Auto-tune** Spark’s shuffle partitions  \n",
    "\n",
    "> Outputs a new DataFrame `df_salted`\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter breakdown\n",
    "\n",
    "- **`col_name=top_skew`**  \n",
    "  The **column name** (string) you want to rebalance.  \n",
    "  Here `top_skew` comes from your skew detector (`skewed[0]`).\n",
    "\n",
    "- **`salt_count=10`**  \n",
    "  Start by slicing your data into **10 buckets** (salt values 0–9).  \n",
    "  ▶️ *Tip:* Pick a power-of-two or something close to your cluster’s cores.\n",
    "\n",
    "- **`output_dir=\"hexa_druid_outputs\"`**  \n",
    "  All visualizations and logs are saved under this folder.  \n",
    "  ▶️ *Tip:* Make this path unique per dataset or run to avoid overwriting.\n",
    "\n",
    "- **`visualize=True`**  \n",
    "  Saves three charts to help you verify the effect:  \n",
    "  - Z-score comparison (before vs after)  \n",
    "  - Boxplot (original vs salted)  \n",
    "  - Histogram with density curves  \n",
    "  ▶️ *Tip:* If you’re running in a headless environment, these still generate PNG files.\n",
    "\n",
    "- **`fine_tune=True`**  \n",
    "  Runs the **AutoParameterTuner** on a **10% sample** of your data to pick a better `salt_count`.  \n",
    "  It tests the supplied counts (`[10, dp, dp*2]`) and chooses the one minimizing the sum-of-squares of partition sizes.  \n",
    "  ▶️ *Tip:* Great for beginners—avoid manual trial-and-error.\n",
    "\n",
    "- **`auto_tune=True`**  \n",
    "  After salting, adjusts `spark.sql.shuffle.partitions` based on your **`target_rows`**.  \n",
    "  Ensures partitions are neither too small (wasted overhead) nor too large (OOM risk).\n",
    "\n",
    "- **`target_rows=1000`**  \n",
    "  The tuner tries to achieve ~**1,000 rows per partition**.  \n",
    "  ▶️ *Tip:* Lower for very skewed data or smaller clusters; increase for big clusters.\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **Tips for Noobs:**  \n",
    "> - **Check your plots!** Open the PNGs under `hexa_druid_outputs/` to visually confirm balance.  \n",
    "> - If your data is small, **reduce** `target_rows` to keep partitions reasonably sized.  \n",
    "> - If salting doesn’t help, try **lowering** `skew_thresh` in Step 6 or **increasing** `salt_count`.  \n",
    "> - Always **version** your `output_dir` (e.g., include a timestamp) to compare different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a71afb67e50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the output index from Step (6)\n",
    "top_skew = skewed[0]\n",
    "\n",
    "hexa = HexaDruid(df_typed, salt_count=10, output_dir=\"hexa_druid_outputs\")\n",
    "df_salted = hexa.apply_smart_salting(\n",
    "    col_name=top_skew,\n",
    "    visualize=True,\n",
    "    fine_tune=True,\n",
    "    auto_tune=True,\n",
    "    target_rows=1000\n",
    ")\n",
    "print(\"Salting complete. Plots saved under hexa_druid_outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829e65ec2b135c2",
   "metadata": {},
   "source": [
    "# Step 9: Repartition on Salted Key\n",
    "\n",
    "(Optional) Force repartition on `\"salted_key\"` to guarantee even distribution.\n",
    "\n",
    "- Use `npartitions` equal to your cluster parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ca4acdd52aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart = hexa.repartition_on_salt(num_partitions=10)\n",
    "print(\"Repartitioned on salted_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b6237c9a10ab2",
   "metadata": {},
   "source": [
    "# Step 10: Show Partition Sizes\n",
    "\n",
    "Compare record counts per partition **before** vs **after** salting.\n",
    "\n",
    "- Helps you verify the skew has been mitigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5e0f725f187d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexa.show_partition_sizes(df_typed, label=\"Before Salting\")\n",
    "hexa.show_partition_sizes(df_repart, label=\"After Salting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318ba95179f4f8e",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Step 11: Detect Primary Key Candidate(s) OR Detect Composite Primary Key Candidates\n",
    "\n",
    "Using your **DRTree** shards, we aggregate:\n",
    "\n",
    "- **Distinct counts**: how many **unique values** exist per column (or combo)\n",
    "- **Null counts**: how many **missing values** per column\n",
    "\n",
    "We compute a **confidence score** for each column or combo:\n",
    "\n",
    "### Method Used: `hexa.detect_keys()`\n",
    "\n",
    "This will **first attempt to find a single-column primary key**.  \n",
    "If none is confidently detected, it will **automatically test combinations of 2–3 columns** for a **composite key**.\n",
    "\n",
    "### Parameter breakdown\n",
    "\n",
    "- **`dr_tree: Optional[DRTree] = None`**  \n",
    "\n",
    "  - A DRTree (Decision Rule Tree) **partitioning the data into logical shards**.  \n",
    "\n",
    "  - This helps analyze primary key uniqueness **within each branch of the data**, which is often more reliable than analyzing the entire dataset at once.\n",
    "\n",
    "  - This is the Decision Root Tree that contains filtering predicates per shard.\n",
    "\n",
    "  - Each \"shard\" helps break your data down into logical partitions, making the analysis scale-aware and context-sensitive.\n",
    "\n",
    "  - If not passed, a fallback tree (1=1) is used to treat the entire dataset as one shard.\n",
    "\n",
    "  ▶️ *Tip:* You get this tree from `schemaVisor()` (Step 5). DRTree is built automatically via HexaDruid.schemaVisor(...)\n",
    "\n",
    "- **`verbose=True`**  \n",
    "  Prints out the **distinct ratio, null ratio, and score** per column (or combo).  \n",
    "  ▶️ *Tip:* Useful to manually verify borderline cases.\n",
    "\n",
    "---\n",
    "### 💡 Tips for Beginners\n",
    "\n",
    "- You **must run `schemaVisor()`** before this step so you have a valid `dr_tree` to pass.\n",
    "- If your dataset has no unique ID column, try to **combine 2 or 3 categorical columns** (e.g. `order_id`, `product_id`, `region`)—this is what composite key detection does automatically.\n",
    "- If you get a `KeyError`, make sure the DRTree isn't empty, and that `df_typed` has the expected columns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3ecfa6ab8c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_result = hexa.detect_keys(dr_tree=dr_tree, verbose=True)\n",
    "\n",
    "if key_result:\n",
    "    print(f\"Detected {key_result['type']} key column(s): {key_result['columns']}, confidence: {key_result['confidence']}\")\n",
    "else:\n",
    "    print(\"No confident primary or composite key detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da488e",
   "metadata": {},
   "source": [
    "# Step 12: Advisor Suggestions\n",
    "\n",
    "The **AutoParameterAdvisor** shows you:\n",
    "\n",
    "- A table of **skew** vs **cardinality**  \n",
    "- Top 3 skew candidates  \n",
    "- Top 3 groupBy candidates  \n",
    "\n",
    "You can then pick interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor = AutoParameterAdvisor(df_typed)\n",
    "skew_cands, cat_cands, metrics_df = advisor.recommend()\n",
    "print(\"Pick from:\", cat_cands)\n",
    "\n",
    "# Instead of letting it prompt\n",
    "df_balanced = HexaDruid(df_typed).apply_smart_salting(col_name=skew_cands[0])\n",
    "df_balanced.groupBy(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309e490",
   "metadata": {},
   "source": [
    "# Step 13: Run Interactive Full Pipeline\n",
    "\n",
    "One command that:\n",
    "\n",
    "1. Prompts you to **choose** skew & groupBy columns  \n",
    "2. Applies **salting**, shows **before/after** timings & partition sizes  \n",
    "\n",
    "Perfect for demos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced = balance_skew(df_typed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14fa190",
   "metadata": {},
   "source": [
    "# Step 14: Stop Spark Session\n",
    "\n",
    "Always clean up when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
