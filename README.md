# HexaDruid ğŸ§ âš¡

[![PyPI version](https://badge.fury.io/py/hexadruid.svg)](https://badge.fury.io/py/hexadruid)
[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**HexaDruid** is an intelligent Spark optimizer designed to tackle value skew, schema bloat, and ambiguous key detection using recursive decision-rule trees and adaptive tuning. It enables smart salting, distributed key detection, and shard-aware schema inference â€” all with PySpark's native DataFrame API.

---

## ğŸ“¦ Installation

```bash
pip install hexadruid
```

---

## ğŸ” Features

- ğŸ“Š Smart salting using IQR or z-score binning
- ğŸŒ² Recursive DRTree-based shard splitting (non-linear DAG logic)
- ğŸ”‘ Primary & Composite Key Detection (UUIDs, strings, hexadecimals)
- ğŸ§  Schema Inference with memory-safe type coercion and metadata hints
- ğŸ§ª Z-score visualizations and partition diagnostics
- âš™ï¸ Auto tuning of salt count and shuffle partitions
- âœ… 100% Spark-native â€” no RDDs, no CLI, no external dependencies

---

## ğŸš€ Quickstart

```python
from hexadruid import HexaDruid

hd = HexaDruid(df)

# Balance skew in a numeric column
df_salted = hd.apply_smart_salting("sales_amount")

# Detect keys (UUIDs, composite, etc.)
key = hd.detect_keys()

# Infer schema and shard tree
typed_df, inferred_schema, dr_tree = HexaDruid.schemaVisor(df)
```

---

## ğŸ“š What Does It Do?

Imagine this DataFrame:

| order_id (UUID) | amount |
|-----------------|--------|
| a12e...         | 500.0  |
| b98c...         | 5000.0 |
| ...             | ...    |

You want to `groupBy("amount")`, but most rows have the same few values â†’ Spark chokes on skew.

### ğŸ”¥ With HexaDruid:

```python
df2 = hd.apply_smart_salting("amount")
```

It:

1. Detects skew using IQR asymmetry or z-score
2. Creates a `salt` column using percentile splits
3. Builds `salted_key = concat(amount, salt)`
4. Repartitions the DataFrame using `salted_key`

âœ… Result: Balanced shuffle across executors.

---

## ğŸŒ³ DRTree (Decision Rule Tree) â€” Deep Dive

HexaDruidâ€™s `drTree()` is not a classifier. It's a recursive shard explainer that builds SQL-compatible predicates based on imbalanced features.

Each node evaluates data distribution and splits the dataset using decision logic on the best skewed column.

### ASCII Tree

```
                [Root: sales_amount]
                        |
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     [amount <= 500]         [amount > 500]
           |                         |
       [Leaf A]                  [Leaf B]
    (shard_1234)              (shard_5678)
```

- **Root**: Skewed feature selected for first split
- **Branches**: Logical split conditions (e.g., amount <= 500)
- **Leaves**: Filtered DataFrames with shard metadata
- **Output**: SQL predicates reusable for schemaVisor, key detection, or isolation

---

## ğŸ”¬ Smart Salting Algorithm

**Input**: Skewed numeric column  
**Goal**: Equalize shuffle load in `groupBy`, `join`

### Steps:

1. **Skew Check**:
   ```python
   z = (x - mean(x)) / std(x)   # or use IQR-based rule
   ```
2. **Percentile Splitting**:
   ```python
   bounds = percentile_approx(x, [0%, 10%, ..., 100%])
   ```
3. **Salt Assignment**:
   ```python
   salt = when((x >= b0) & (x < b1), 0).when(...)...
   ```
4. **Salted Key & Repartition**:
   ```python
   salted_key = concat_ws("_", col("x"), col("salt"))
   df.repartition(N, salted_key)
   ```

ğŸ¯ Auto-tuned salt count if not provided.

---

## ğŸ”‘ Key Detection (UUIDs, Alphanumerics, Composite)

```python
key_info = hd.detect_keys()
```

### Primary Key Scoring:

```python
score = approx_count_distinct(col) / total_rows - null_ratio
if score â‰¥ 0.99:
    â†’ confident primary key
```

### Composite Key:

```python
combo = concat_ws("_", col1, col2, ...)
score = approx_count_distinct(combo) / total_rows - null_ratio
if score â‰¥ 0.99:
    â†’ confident composite key
```

ğŸ’¡ Supports detection of:
- UUIDs
- Alphanumeric IDs
- Hexadecimal fields
- Concatenated business keys

---

## ğŸ§  schemaVisor (Schema + Tree Inference)

```python
typed_df, inferred_schema, tree = HexaDruid.schemaVisor(df)
```

### Features:

- Optimizes column types: string â†’ varchar(x), float â†’ int if safe
- Adds logical metadata (nullable, min_len, max_len)
- Connects schema + tree for shard-specific schema enforcement

---

## ğŸ§ª Testing

```bash
pytest tests/
```

Mocked SparkSession and sample DataFrames are included.

---

## ğŸ§± Project Structure

```
hexadruid/
â”œâ”€â”€ core.py                # Main interface
â”œâ”€â”€ skew_balancer.py       # Salting + distribution logic
â”œâ”€â”€ key_detection.py       # Primary + composite key analysis
â”œâ”€â”€ schema_optimizer.py    # schemaVisor and type casting
â”œâ”€â”€ drtree.py              # DRTree recursive engine
â”œâ”€â”€ advisor.py             # Parameter tuning module
â””â”€â”€ utils.py               # Plotting, IQR, logging
```

---

## ğŸ›£ï¸ Roadmap

- [ ] Delta Lake & Apache Iceberg compatibility
- [ ] CLI & REST API
- [ ] Auto JSON export of tree branches for audit
- [ ] JupyterLab plugin for visual shard insight

---

## ğŸ“„ License

MIT License

---

## ğŸ¤ Contributing

Pull requests, issues, and ideas are welcome!  
HexaDruid is evolving â€” your insights shape its intelligence.
