{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a86138",
   "metadata": {},
   "source": [
    "## üå≤ Full HexaDruid Feature Showcase\n",
    "\n",
    " We‚Äôll walk through every top-level API with tips, parameter explanations, and best practices:\n",
    "\n",
    "  1. **Schema inference & DRTree** (`schemaVisor` / `infer_schema`)\n",
    " 2. **Skew column discovery** (`detect_skew`)\n",
    " 3. **Key detection** (`detect_keys`)\n",
    " 4. **Parameter recommendations** (`AutoParameterAdvisor`)\n",
    " 5. **Fast heavy-hitter salting** (`apply_smart_salting`)\n",
    " 6. **One-liner optimization** (`simple_optimize`)\n",
    " 7. **Before/after visualization** (`visualize_salting`)\n",
    " 8. **Interactive tuning** (`interactive_optimize`)\n",
    "\n",
    " üí° _Tip for Noobs_: Run each cell in order. Adjust `sample_frac`, `threshold`, and `salt_count` to fit your data size and cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from hexadruid import (\n",
    "    HexaDruid,\n",
    "    infer_schema,\n",
    "    detect_skew,\n",
    "    detect_keys,\n",
    "    AutoParameterAdvisor,\n",
    "    simple_optimize,\n",
    "    visualize_salting,\n",
    "    interactive_optimize)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HexaDruid Full Demo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b81ceb",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2) Spark Configuration\n",
    "\n",
    " - **`spark.sql.shuffle.partitions`** should match your expected salt buckets to avoid tiny tasks.\n",
    " - **`spark.default.parallelism`** controls default partitions for RDD operations (e.g. heavy-hitter scan).\n",
    "\n",
    " **Best Practice:** set both to `min(salt_count, cluster_cores)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune shuffle / parallelism for 8 buckets\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df877268",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è 3) Create a Skewed DataFrame\n",
    "\n",
    " We simulate 100 K rows:\n",
    " - 80 % of `user_id` = \"A\" ‚Üí a hot key\n",
    " - `amount` cycles 0‚Äì99 (uniform)\n",
    "\n",
    " _Dev Tip_: Use `.limit()` for quick tests on subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d583b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"A\" if i % 5 != 0 else f\"U{i%10}\", float(i % 100))\n",
    "        for i in range(100_000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"amount\"])\n",
    "\n",
    "print(\"Total rows:\", df.count())\n",
    "\n",
    "df.groupBy(\"user_id\").count().orderBy(\"count\", ascending=False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46751afd",
   "metadata": {},
   "source": [
    "### üìê 4) Schema Inference & DRTree\n",
    "\n",
    " **`infer_schema(df, sample_frac)`**  \n",
    " - **`sample_frac`**: fraction of rows to collect (e.g. 0.01 = 1%)  \n",
    " - Builds a safe `StructType` via driver-side regex/JSON sniffing  \n",
    " - Returns `(typed_df, schema, dr_tree)`  \n",
    "\n",
    " **Output**:  \n",
    " - `schema.simpleString()` shows each column‚Äôs inferred Spark type  \n",
    " - `dr_tree.to_dict()` shows a trivial ‚Äúall‚Äù branch if no skew detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d098ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "typed_df, schema, dr_tree = infer_schema(df, sample_frac=0.01)\n",
    "\n",
    "print(\"‚úÖ Inferred schema:\", schema.simpleString())\n",
    "\n",
    "print(\"üå≤ DRTree:\", dr_tree.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d730c",
   "metadata": {},
   "source": [
    "### üîç 5) Skew Column Discovery\n",
    "\n",
    " **`detect_skew(df, threshold, top_n)`**  \n",
    " - **`threshold`**: minimum IQR-based skew to include (0.0 to get any skew)  \n",
    " - **`top_n`**: return up to N columns by descending skew score  \n",
    "\n",
    " _Noobs_: set `threshold=0.0` to always get `top_n` candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_cols = detect_skew(df, threshold=0.0, top_n=2)\n",
    "\n",
    "print(\"üîç Top skewed columns:\", skew_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f8dc2",
   "metadata": {},
   "source": [
    "### üîë 6) Key Detection\n",
    "\n",
    " **`detect_keys(df, threshold, max_combo)`**  \n",
    " - **`threshold`**: uniqueness ratio (distinct‚Äìnull)/total to qualify  \n",
    " - **`max_combo`**: max columns to test for composite keys  \n",
    "\n",
    " _Pro Tip_: lower `threshold` (e.g. 0.01) when you know keys aren‚Äôt 100 % unique.  \n",
    " By default it returns **the best** candidate if none meet the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91440bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = detect_keys(df, threshold=0.01, max_combo=2)\n",
    "\n",
    "print(\"üîë Detected key candidate(s):\", keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7bb99",
   "metadata": {},
   "source": [
    "### üìä 7) Parameter Recommendations\n",
    "\n",
    " **`AutoParameterAdvisor(df, skew_top_n, cat_top_n)`**  \n",
    " - Samples up to `max_sample` rows for lightning-fast metrics  \n",
    " - Recommends top skewed numeric & low-cardinality categorical columns  \n",
    " - Returns `(skew_cands, cat_cands, metrics_df)`  \n",
    "\n",
    " **Metrics Table**:  \n",
    " - `skew`: IQR-based score  \n",
    " - `distinct` / `nulls` count on the sample  \n",
    "\n",
    " _Developer Tip_: adjust `sample_frac` or `max_sample` in the class if you need larger samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09310b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor = AutoParameterAdvisor(df, skew_top_n=2, cat_top_n=2)\n",
    "\n",
    "skew_cands, cat_cands, metrics_df = advisor.recommend()\n",
    "\n",
    "print(\"Numeric candidates:\", skew_cands)\n",
    "\n",
    "print(\"Categorical candidates:\", cat_cands)\n",
    "\n",
    "metrics_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14347e77",
   "metadata": {},
   "source": [
    "### ‚ö° 8) Fast Heavy-Hitter Salting (Auto)\n",
    "\n",
    " **`apply_smart_salting(col_name=None, salt_count=None)`**  \n",
    " - **Auto-detects** the single most skewed column if `col_name=None`  \n",
    " - **Auto-sets** salt_count = `sparkContext.defaultParallelism` if `salt_count=None`  \n",
    " - **Heavy hitters** (> total/salt_count) get random buckets  \n",
    " - **Others** are hashed via `pmod(hash(key), salt_count)`  \n",
    " - Single full-table shuffle ‚Üí near-linear performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = HexaDruid(df)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_auto = hd.apply_smart_salting()  # no args = auto mode\n",
    "\n",
    "print(f\"\\n‚ö° Auto salting took {time.time() - t0:.2f}s\")\n",
    "\n",
    "df_auto.groupBy(\"salt\").count().orderBy(\"salt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3894fa",
   "metadata": {},
   "source": [
    "### üîÑ 9) One-Liner Optimize for Beginners\n",
    "\n",
    " **`simple_optimize(df, skew_col, sample_frac, salt_count)`**  \n",
    " wraps `infer_schema` + `apply_smart_salting` in one call.  \n",
    " Returns the salted & repartitioned DataFrame.  \n",
    "\n",
    " _Example_: rebalance on `\"user_id\"` with 5 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "df_simple = simple_optimize(\n",
    "    df,\n",
    "    skew_col=\"user_id\",\n",
    "    sample_frac=0.005,  # 0.5% sample for type sniffing\n",
    "    salt_count=5\n",
    ")\n",
    "print(f\"\\nüîß simple_optimize took {time.time() - t1:.2f}s\")\n",
    "\n",
    "df_simple.groupBy(\"salt\").count().orderBy(\"salt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302a4e3",
   "metadata": {},
   "source": [
    "### üìà 10) Before/After Visualization\n",
    "\n",
    " **`visualize_salting(df, skew_col, salt_count)`**  \n",
    " - Prints original skew distribution  \n",
    " - Applies heavy-hitter salting  \n",
    " - Prints new bucket counts  \n",
    "\n",
    " Great for quick sanity checks without manual `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_salting(\n",
    "    df,\n",
    "    skew_col=\"user_id\",\n",
    "    salt_count=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f350d98",
   "metadata": {},
   "source": [
    "### üë©‚Äçüíª 11) Interactive Tuning\n",
    "\n",
    " **`interactive_optimize(df, sample_frac, skew_top_n, cat_top_n)`**  \n",
    " 1. Shows recommended skew & categorical columns with metrics  \n",
    " 2. Prompts you to **pick any** column  \n",
    " 3. Applies heavy-hitter salting on your choice  \n",
    " 4. **Always** pick a low-cardinality column (e.g low distinct)\n",
    "\n",
    " Perfect for beginners who want guidance without writing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüë©‚Äçüíª Interactive optimize‚Äîpick any column:\")\n",
    "\n",
    "df_inter = interactive_optimize(df)\n",
    "\n",
    "print(\"\\n‚úÖ Final salt distribution:\")\n",
    "\n",
    "df_inter.groupBy(\"salt\").count().orderBy(\"salt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f5f01",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary & Best Practices\n",
    "\n",
    " - **Cache** intermediate DataFrames only when reused.  \n",
    " - **Align** `spark.sql.shuffle.partitions` to your salt buckets.  \n",
    " - **Sample** wisely‚Äîsmall fractions for large tables.  \n",
    " - **Lower thresholds** when exploring skew and keys.  \n",
    " - **Use heavy-hitter salting** to outperform multi-pass quantiles.  \n",
    "\n",
    " HexaDruid turns dozens of lines into self‚Äêtuning, visual, Spark-native calls‚Äîperfect for noobs and experts alike! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
